{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from transformers import OFATokenizer, OFAModel, OFAConfig\n",
    "# from generate import sequence_generator\n",
    "from transformers.models.ofa.generate import sequence_generator\n",
    "# from OFA.fairseq.fairseq import sequence_generator\n",
    "\n",
    "\n",
    "mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
    "resolution = 480\n",
    "patch_resize_transform = transforms.Compose([\n",
    "    lambda image: image.convert(\"RGB\"),\n",
    "    transforms.Resize((resolution, resolution), interpolation=Image.BICUBIC),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "ckpt_dir = '/home/scur1045/FACT-project/OFA/OFA-HF-large-model'\n",
    "tokenizer = OFATokenizer.from_pretrained(ckpt_dir)\n",
    "# tokenizer = OFATokenizer(vocab_file=f'{ckpt_dir}/vocab.json', merges_file=f'{ckpt_dir}/merges.txt')\n",
    "\n",
    "\n",
    "txt = \" what does the image describe?\"\n",
    "inputs = tokenizer([txt], return_tensors=\"pt\").input_ids\n",
    "# path_to_image = '/home/scur1045/FACT-project/HVV_EXPGEN_DATASET/Train_Val_Images/covid_memes_2076.png'\n",
    "path_to_image = '/home/scur1045/FACT-project/HVV_EXPGEN_DATASET/Train_Val_Images/memes_1452.png'\n",
    "# path_to_image = '/home/scur1045/FACT-project/img_test.png'\n",
    "img = Image.open(path_to_image)\n",
    "patch_img = patch_resize_transform(img).unsqueeze(0)\n",
    "\n",
    "\n",
    "model = OFAModel.from_pretrained(ckpt_dir, use_cache=True)\n",
    "generator = sequence_generator.SequenceGenerator(\n",
    "                    tokenizer=tokenizer,\n",
    "                    beam_size=5,\n",
    "                    max_len_b=16, \n",
    "                    min_len=0,\n",
    "                    no_repeat_ngram_size=3,\n",
    "                )\n",
    "\n",
    "data = {}\n",
    "data[\"net_input\"] = {\"input_ids\": inputs, 'patch_images': patch_img, 'patch_masks':torch.tensor([True])}\n",
    "gen_output = generator.generate([model], data)\n",
    "gen = [gen_output[i][0][\"tokens\"] for i in range(len(gen_output))]\n",
    "\n",
    "# using the generator of huggingface version\n",
    "model = OFAModel.from_pretrained(ckpt_dir, use_cache=False)\n",
    "gen = model.generate(inputs, patch_images=patch_img, num_beams=5, no_repeat_ngram_size=3) \n",
    "\n",
    "print(tokenizer.batch_decode(gen, skip_special_tokens=True))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
